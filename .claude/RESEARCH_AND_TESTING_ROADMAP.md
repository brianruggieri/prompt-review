# AI Agent "Hello World", Benchmarking, and Local Testing Roadmap

## Executive Summary

This document ties together:
1. **AI Agent Hello World patterns** â€” What canonical tests look like
2. **Academic benchmarks** â€” How existing systems measure code generation quality
3. **Prompt ambiguity metrics** â€” How to detect and classify vagueness
4. **Mock data strategy** â€” How to stress test prompt-review systematically
5. **Human local testing plan** â€” How to run real validation locally

**Status:** âœ… Research complete, hello world demo working, ready to implement tiers

---

## Part 1: AI Agent "Hello World" Patterns

### What It Means

AI agent "hello world" = **simplest test that demonstrates the entire system end-to-end**.

For prompt-review, this is **the email validation prompt** that shows:
- System detects vagueness
- Gate blocks/warns on ambiguity
- Scoring correlates with clarity
- Learning system tracks iterations

### Our Implementation

**Hello World:** Email validation prompt across 3 clarity levels

```
V1 (Vague):   "Write a function that validates email addresses"
              â†’ Score: 3.5, Severity: major, Gate: warn
              â†’ 5 findings across clarity, security, testing

V2 (Medium):  "Write Python function using regex pattern [...], return bool [...]"
              â†’ Score: 6.8, Severity: minor, Gate: proceed
              â†’ 2 findings (improved by 3)

V3 (Clear):   "Write validate_email(email: str) -> bool with exact spec [...]"
              â†’ Score: 8.9, Severity: nit, Gate: proceed
              â†’ 1 finding (improved by 1)
```

**Demo Status:** âœ… PASSING
- All 8 validation tests pass
- Shows score progression 3.5 â†’ 6.8 â†’ 8.9
- Finding count decreases monotonically
- Gate actions predictable and correct

**Run it:**
```bash
node tests/hello-world-demo.test.cjs
# Output: PASSED âœ“ (8/8 tests)
```

---

## Part 2: Benchmarking Foundations

### Academic Benchmarks Reviewed

| Benchmark | Relevance | Key Finding |
|-----------|-----------|---|
| **HumanEval** | Low complexity baseline | 164 simple problems; doesn't capture real ambiguity |
| **MBPP** | Entry-level patterns | 974 Python problems; shows ambiguity is detectable |
| **CodeXGLUE** | Multi-task diversity | 10 tasks across 14 datasets; emphasizes prompt diversity |
| **SWE-bench** | Real-world complexity | 500 real GitHub issues; difficulty tied to scope ambiguity |
| **SWE-bench Verified** | Gold standard | 500 human-validated; shows 15-60 min tasks clearest |
| **TAU-bench** | Tool use | Shows agentic systems need clear scope |

### Key Research Findings

1. **SpecFix Study (May 2025):** 43.58% of code generation prompts need repair
   - Improvement when repaired: 30.9% in Pass@1
   - Repairs transfer across models: 10.48% improvement on other LLMs
   - **Implication:** Ambiguity detection directly improves outcomes

2. **Ambiguous Questions Study (NeurIPS 2024):** 86.25% detection accuracy possible
   - Path kernel methods work well
   - Multiple valid interpretations = high ambiguity
   - **Implication:** Vagueness is detectable and measurable

3. **Real-World SWE-Bench Analysis:** Scope ambiguity = difficulty predictor
   - 15-60 min tasks: Clear performance separation between models
   - 4+ hour tasks: Only SOTA solves
   - **Implication:** Clear prompts correlate with solvability

### Our Relevance

prompt-review sits at the **input side** of this pipeline:
- **Before:** Vague prompt â†’ ambiguous code generation â†’ bugs
- **Our role:** Detect and help clarify before generation
- **After:** Clearer prompt â†’ better code generation â†’ fewer bugs

---

## Part 3: Prompt Ambiguity Detection

### The 5 Dimensions of Ambiguity

Based on research, we test across these dimensions:

#### **1. Vagueness** (Imprecise verbs, undefined terms)
```
Vague:   "Fix the bug"
Clear:   "Fix login failure: password reset endpoint returns 401 instead of 302 redirect"
```

#### **2. Scope Ambiguity** (Undefined boundaries)
```
Vague:   "Improve authentication"
Clear:   "Add 2FA to login: SMS verification, max 3 attempts, 5-min expiry"
```

#### **3. Output Specification** (What should be produced)
```
Vague:   "Return success or failure"
Clear:   "Return { success: bool, error: string | null, data: User | null }"
```

#### **4. Context Richness** (Background information)
```
Vague:   "Refactor the pricing"
Clear:   "Refactor calculatePrice() to use PricingService v2.1 API (see docs/...)"
```

#### **5. Implicit Assumptions** (Unstated requirements)
```
Vague:   "Make it faster"
Clear:   "Reduce latency to < 100ms (p99), using Redis caching, not SQL changes"
```

### Detection Heuristics

Clarity reviewer currently detects:
- âœ… Vague verbs (optimize, improve, fix, clean up without measurable criteria)
- âœ… Missing output format
- âœ… Ambiguous scope
- âœ… Missing success criteria
- âœ… Implicit assumptions

**Score interpretation:**
- 0â€“3: Poor (entirely vague)
- 4â€“6: Needs work (significant ambiguity)
- 7â€“9: Good (minor improvements)
- 10: Excellent (precise and specific)

---

## Part 4: Mock Data Strategy

### 3-Phase Implementation

**Phase 1: Tier 1 (Basic Coverage)**
- 5 ambiguity dimension scenarios (5 prompts each) = 25 base prompts
- 4 real-world categories Ã— 3 clarity levels = 12 prompts
- Total: ~350 mock reviews when injected
- **Expected outcome:** All dimensions clearly separated (rÂ² > 0.85)

**Phase 2: Tier 2 (Stress Tests)**
- Cascading refinements (show score improvement)
- Parallel ambiguity (same task, different descriptions)
- SWE-bench adapted scenarios (15 real GitHub issues)
- **Expected outcome:** Monotonic score improvement, findings decrease

**Phase 3: Tier 3 (Comprehensive Benchmarking)**
- HumanEval-style mini suite (10 problems)
- MBPP-style entry-level suite (20 problems)
- Comprehensive benchmarking dashboard
- **Expected outcome:** Complete validation across all metrics

### File Structure

```
mock-data/
â”œâ”€ hello-world.cjs                 # âœ… READY
â”œâ”€ tier-1/
â”‚  â”œâ”€ vagueness-spectrum.cjs       # (TODO: implement)
â”‚  â”œâ”€ scope-ambiguity.cjs          # (TODO: implement)
â”‚  â””â”€ ...
â”œâ”€ tier-2/
â”‚  â”œâ”€ cascading-refinements.cjs    # (TODO: implement)
â”‚  â””â”€ ...
â””â”€ tier-3/
   â”œâ”€ humaneval-mini.cjs           # (TODO: implement)
   â””â”€ ...

scripts/
â”œâ”€ inject-mock-reviews.cjs         # (TODO: implement)
â”œâ”€ run-stress-test.cjs             # (TODO: implement)
â”œâ”€ test-dashboard.cjs              # (TODO: implement)
â””â”€ ...
```

### What Gets Generated

When you run the mock data generation:
```bash
node scripts/generate-mock-data.cjs

# Output:
# - 350 audit log entries (Phase 1)
# - 50+ review findings
# - Varied clarity scores (0.5 â†’ 10)
# - Multiple severity levels
# - Realistic rejection patterns
```

---

## Part 5: Local Testing Plan

### Quick Start (30 minutes)

```bash
# 1. Run hello world (already works!)
node tests/hello-world-demo.test.cjs
# Output: PASSED âœ“ (demonstrates entire system)

# 2. (TODO) Generate Tier 1 mock data
node scripts/generate-mock-data.cjs --tier 1

# 3. (TODO) Inject into audit logs
node scripts/inject-mock-reviews.cjs --tier 1 --count 50

# 4. Check system sees data
node index.cjs --stats
# Output: [Dashboard showing 50 reviews, distributions, metrics]

# 5. Verify learning system
node adapt.cjs 30
# Output: [Precision per role, suggested weight changes]

# 6. See the full dashboard
node scripts/test-dashboard.cjs
# Output: [Visual summary: all tests passed, what works, what doesn't]
```

### Comprehensive Testing (2-3 hours)

**Day 1: Tier 1 Validation**
- Run all 5 dimension scenarios
- Verify clarity score separation (rÂ² > 0.85)
- Check gate accuracy (major findings caught)
- Output: `results/tier-1-validation.json`

**Day 2: Tier 2 Stress Tests**
- Run cascading refinements (score improves monotonically)
- Run parallel ambiguity (same task, different clarity)
- Run SWE-bench adapted issues (realistic scenarios)
- Output: `results/tier-2-validation.json`

**Day 3: Tier 3 Benchmarking**
- Run HumanEval mini suite (10 problems)
- Run MBPP mini suite (20 problems)
- Run comprehensive benchmarking
- Output: `results/tier-3-validation.json` + `results/benchmark-report.html`

### Metrics Dashboard

**Target Output:**

```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  MOCK DATA TEST DASHBOARD
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Phase 1: Tier 1 Spectrum Coverage
  âœ“ Vagueness dimension (5 prompts)     Score: 0.5-9.5
  âœ“ Scope ambiguity (5 prompts)         Coverage: 100%
  âœ“ Output spec (5 prompts)             Finding accuracy: 96%
  âœ“ Context richness (5 prompts)        Gate detection: Perfect
  âœ“ Implicit assumptions (5 prompts)    Correlation: 0.94

Phase 2: Cascading Refinements
  âœ“ V1â†’V2 improvement: +2.8 points
  âœ“ V2â†’V3 improvement: +2.1 points
  âœ“ Total improvement: ~5.5 points

Phase 3: Benchmarking
  âœ“ HumanEval suite: 10/10 parsed
  âœ“ MBPP suite: 20/20 parsed
  âœ“ SWE-bench adapted: 15/15 accurate

Stress Tests
  âœ“ Edge cases: 6/6 handled
  âœ“ Concurrent: 100 submissions, 99.2% consistent
  âœ“ Learning system: Weight changes valid

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
OVERALL: 54/54 tests passed | System ready
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### "Gotcha" Tests (Known Failure Modes)

Catch potential issues:

```bash
# Test 1: Hash tampering detection
node scripts/tamper-test.cjs --edit-entry 0 --change-score 10
# Expected: verifyAuditEntry() catches it, entry skipped

# Test 2: Division by zero in precision_strict
node scripts/inject-mock-reviews.cjs --invalid-rejected 0 --proposed 5
# Expected: No NaN, precision_strict = 1.0 (correct handling)

# Test 3: Dominant role detection
node scripts/stress-test.cjs --dominant-role security --weight 10
# Expected: Fairness warning in review output

# Test 4: Concurrent submissions
node scripts/stress-test.cjs --concurrent 100 --randomness
# Expected: All consistent, no race conditions
```

---

## Part 6: Recommended Execution Order

### Week 1: Foundation & Hello World
- âœ… **Done:** Hello world demo (passes)
- âœ… **Done:** Research and strategy (research complete)
- ðŸ“‹ **Next:** Implement Tier 1 mock data generation
- ðŸ“‹ **Next:** Implement injection script
- ðŸ“‹ **Next:** Run first full cycle (generate â†’ inject â†’ validate)

### Week 2: Comprehensive Validation
- Tier 2 cascading refinements
- Tier 2 parallel ambiguity
- Tier 2 SWE-bench scenarios
- Stress tests (edge cases, performance)

### Week 3: Benchmarking & Reporting
- Tier 3 HumanEval suite
- Tier 3 MBPP suite
- Comprehensive benchmarking dashboard
- Generate final report

### Ongoing: Monthly Validation
- Run full suite monthly
- Track trends (scores, gate accuracy, learning system)
- Identify regressions
- Update mock data as tool evolves

---

## Part 7: Expected Benefits

### What We'll Prove

âœ… **Clarity scores predict gate actions**
- Major findings â†’ block/warn
- Minor findings â†’ proceed
- Correlation rÂ² > 0.85

âœ… **System detects real ambiguity**
- Vague prompts (0â€“3) clearly separated from clear (8â€“10)
- Findings decrease monotonically with refinement
- Gate actions predictable

âœ… **Learning system works**
- Weight adaptation improves precision
- Coverage metrics detect "plays it safe" reviewers
- Post-adaptation precision improves

âœ… **System handles stress**
- 100 concurrent submissions
- 10,000 word prompts
- Edge cases (multilingual, code-embedded, contradictions)
- 50 weight change cycles

âœ… **Reproducible across benchmarks**
- HumanEval-style problems work
- MBPP-style problems work
- SWE-bench real issues work
- Results transfer to user's real prompts

---

## Part 8: Files to Implement

**High Priority (Week 1):**
1. `scripts/generate-mock-data.cjs` â€” Create all tier 1 scenarios
2. `scripts/inject-mock-reviews.cjs` â€” Write audit log entries
3. `tests/tier-1-validation.test.cjs` â€” Verify all 5 dimensions
4. `scripts/test-dashboard.cjs` â€” Visual summary

**Medium Priority (Week 2):**
5. `mock-data/tier-2/*.cjs` â€” Cascading, parallel, SWE-bench scenarios
6. `scripts/run-stress-test.cjs` â€” Edge case harness
7. `tests/tier-2-validation.test.cjs` â€” Verify improvements

**Lower Priority (Week 3):**
8. `mock-data/tier-3/*.cjs` â€” HumanEval/MBPP suites
9. `scripts/benchmark-report.cjs` â€” Generate HTML report
10. `scripts/monthly-benchmark.sh` â€” Automated validation

---

## Part 9: Success Criteria

| Criterion | Target | Status |
|-----------|--------|--------|
| Hello world passes | 8/8 tests | âœ… DONE |
| Tier 1 separates dimensions | rÂ² > 0.85 | ðŸ“‹ TODO |
| Score improves monotonically | 0.5â†’9.5 over iterations | ðŸ“‹ TODO |
| Gate accuracy | 95%+ | ðŸ“‹ TODO |
| Stress tests pass | 20/20 edge cases | ðŸ“‹ TODO |
| Learning system validates | Weight changes correlate with precision | ðŸ“‹ TODO |
| Dashboard renders | All metrics visible | ðŸ“‹ TODO |
| Monthly benchmark | Automated, trending | ðŸ“‹ TODO |

---

## Summary: What You Have Now

âœ… **Research:** Complete (HumanEval, MBPP, SWE-bench, SpecFix, ambiguity detection)
âœ… **Strategy:** 6-phase implementation plan documented
âœ… **Hello World:** Working demo (email validation, 3 clarity levels, 8/8 tests)
âœ… **Roadmap:** Clear execution path for you to follow

## What's Next for You

1. **Implement Tier 1 generation** (Week 1)
   - Run `node scripts/generate-mock-data.cjs --tier 1`
   - Inject into audit logs
   - Run full validation
   - See first complete test cycle

2. **Add Tier 2 stress tests** (Week 2)
   - Cascading refinements
   - Parallel ambiguity
   - Real SWE-bench scenarios

3. **Comprehensive benchmarking** (Week 3)
   - HumanEval/MBPP suites
   - Dashboard reporting
   - Monthly automation

4. **Human local testing** (Ongoing)
   - Run test suite manually
   - Verify system behavior
   - Catch edge cases
   - Validate learning

---

## Files Reference

**Just Created:**
- `.claude/MOCK_DATA_STRATEGY.md` â€” Complete 6-phase plan
- `.claude/RESEARCH_AND_TESTING_ROADMAP.md` â€” This file
- `mock-data/hello-world.cjs` â€” âœ… Ready
- `tests/hello-world-demo.test.cjs` â€” âœ… Ready (PASSING)

**To Implement:**
- `scripts/generate-mock-data.cjs` â€” Generate Tier 1-3 data
- `scripts/inject-mock-reviews.cjs` â€” Write to audit logs
- `tests/tier-*.test.cjs` â€” Validation suites
- `scripts/test-dashboard.cjs` â€” Visual metrics
- `mock-data/tier-*/` â€” All scenario files

---

**Status:** Ready to execute. Start with Week 1 implementation.
