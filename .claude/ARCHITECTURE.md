# prompt-review Architecture & Learning Flow

Complete visualization of the prompt-review plugin: single-run flows, multi-run learning loops, and knowledge evolution.

---

## Single-Run Flow (One Prompt Review)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          PROMPT REVIEW PIPELINE                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

USER INPUT
   â”‚
   â”œâ”€ HOOK: Prompt submitted (CLI, editor hook, real-time plugin)
   â”‚  â””â”€ Trigger: "!!!" in context OR subscription mode OR API call
   â”‚
   â”œâ”€ EXTRACT: Parse prompt, project context, stack
   â”‚
   â”œâ”€ COMPUTE: SHA256 hash (first 12 hex chars) for deduplication/tracking
   â”‚
   â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: FAN-OUT REVIEWERS (Parallel Specialist Review)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DETERMINE ACTIVE REVIEWERS (based on config + project type)
   â”‚
   â”œâ”€ domain_sme       (business logic, requirements clarity)
   â”œâ”€ security         (auth, secrets, permissions, data protection)
   â”œâ”€ testing          (edge cases, test coverage, validation)
   â”œâ”€ clarity          (ambiguity, jargon, structure)
   â”œâ”€ frontend_ux      (UI/UX concerns - optional)
   â”œâ”€ documentation    (doc completeness - optional)
   â”‚
   â–¼ (ALL PARALLEL VIA Promise.all)

   EACH REVIEWER:
   â”œâ”€ Receives: prompt, previous findings (if any), project context
   â”œâ”€ Runs: LLM call with role-specific system prompt
   â”œâ”€ Returns: { findings: [{ finding_id, severity, issue, ... }], score: 0-10 }
   â”‚
   â–¼

COLLECT CRITIQUES: Array of { reviewer_role, findings, score, ... }

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: OPTIONAL DEBATE (If Conflicts Detected)                            â”‚
â”‚          [Only if debate.enabled=true AND conflicts exist]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

CONFLICT DETECTION: Scan critiques for contradictory operations
   â”‚
   â”œâ”€ security says: "AddConstraint(no-env-access)"
   â”œâ”€ domain_sme says: "RemoveConstraint(no-env-access)" â† CONFLICT!
   â”‚
   â–¼

SELECT DEBATE PAIRS (max 2 pairs, choosing strongest conflicts)
   â”‚
   â–¼

DEBATE ROUND (for each pair):
   â”œâ”€ A's initial argument (2-3 sentences on their position)
   â”œâ”€ B's counter-argument (2-3 sentences on their position)
   â”œâ”€ A's rebuttal (address B's points)
   â”œâ”€ B's rebuttal (address A's points)
   â”‚
   â–¼

JUDGE EVALUATION (Claude Sonnet):
   â”œâ”€ Score each role's argument quality (0-10)
   â”œâ”€ Label argument strengths (specific, evidence-backed, etc.)
   â”œâ”€ Extract policy signal (insight for improving that role's prompt)
   â”œâ”€ Declare winner (role_a, role_b, or tie)
   â”‚
   â–¼

DEBATE LOG RECORDED (but does NOT alter merge output)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: MERGE CRITIQUES (Composite Decision Making)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COMPUTE COMPOSITE SCORE:
   â”œâ”€ Weight each reviewer by configured weight (e.g., security: 1.5)
   â”œâ”€ Higher weight = more influence on final score
   â”œâ”€ Weighted average across all reviewers
   â”‚
   â–¼

MERGE OPERATIONS (extract editing suggestions):
   â”œâ”€ Aggregate all findings into list of operations:
   â”‚  â”œâ”€ AddGuardrail(description)
   â”‚  â”œâ”€ RemoveConstraint(name)
   â”‚  â”œâ”€ RephraseSentence(section, old, new)
   â”‚  â”œâ”€ AddSection(title, content)
   â”‚  â””â”€ etc.
   â”‚
   â”œâ”€ Prioritize by severity (blocker > important > minor)
   â”œâ”€ Deduplicate similar changes
   â”œâ”€ Resolve conflicts via debate winner (if debate ran)
   â”‚
   â–¼

FINAL FINDINGS LIST (ready for user approval/editing)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 4: RENDER & PRESENT (User-Facing Output)                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BUILD EDITOR BLOCK:
   â”œâ”€ Formatted diff showing: [BEFORE] vs [AFTER] edits
   â”œâ”€ Organized by category (security, clarity, testing, etc.)
   â”œâ”€ Severity indicators (ğŸ”´ blocker, ğŸŸ¡ important, ğŸŸ¢ minor)
   â”œâ”€ Individual finding summaries with reviewer + reasoning
   â”‚
   â–¼

PRESENT TO USER:
   â”œâ”€ "Accept" â€” apply all changes to prompt
   â”œâ”€ "Edit" â€” review and customize changes before applying
   â”œâ”€ "Reject" â€” discard suggestions
   â”‚
   â–¼ (User interaction)

RECORD USER DECISION (outcome: approved|edited|rejected)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 5: AUDIT LOG & LEARNING (Permanent Record)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

WRITE AUDIT ENTRY:
   â”œâ”€ timestamp: ISO 8601
   â”œâ”€ project: project name
   â”œâ”€ original_prompt_hash: sha256(prompt)[0:12]
   â”œâ”€ reviewers_active: [domain_sme, security, ...]
   â”œâ”€ findings_detail: [{ reviewer_role, finding_id, severity, issue, op, target }]
   â”œâ”€ suggestions_accepted: [IDs of findings user approved]
   â”œâ”€ suggestions_rejected: [IDs of findings user rejected]
   â”œâ”€ reviewer_stats: { [role]: { proposed, accepted, rejected } }
   â”œâ”€ debate_log: { ran, pairs, judge_feedback, cost } (if debate ran)
   â”œâ”€ composite_score: 0-10 (overall quality)
   â”œâ”€ cost: { input_tokens, output_tokens, usd }
   â”œâ”€ duration_ms: execution time
   â”œâ”€ outcome: approved|edited|rejected
   â””â”€ timestamp_finalized: when user made decision

FILE: ~/.claude/plugins/prompt-review/logs/YYYYMMDD.jsonl
(One JSON line per review)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          END OF SINGLE RUN                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Multi-Run Learning Loop (Continuous Improvement)

```
WEEK 1-2: ACCUMULATION PHASE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Run 1: Prompt for API v2      â”‚
â”‚  â”œâ”€ security: 8/10             â”‚
â”‚  â”œâ”€ testing: 5/10 â† needs work â”‚
â”‚  â”œâ”€ outcome: approved          â”‚
â”‚  â””â”€ findings_accepted: [2]     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Run 2: Prompt for webhook     â”‚
â”‚  â”œâ”€ security: 9/10             â”‚
â”‚  â”œâ”€ testing: 3/10 â† needs work â”‚
â”‚  â”œâ”€ outcome: edited            â”‚
â”‚  â””â”€ findings_accepted: [1, 5]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Run 3: Prompt for scheduler   â”‚
â”‚  â”œâ”€ security: 7/10             â”‚
â”‚  â”œâ”€ testing: 4/10 â† needs work â”‚
â”‚  â”œâ”€ outcome: approved          â”‚
â”‚  â””â”€ findings_accepted: [2, 3]  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
[... more runs accumulate ...]

AFTER 5+ REVIEWS WITH OUTCOMES:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 1: ANALYSIS (via `node adapt.cjs` or `/prompt-review:adapt`)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

READ AUDIT LOGS (last 30 days):
   â”‚
   â”œâ”€ Find all entries with outcome (approved|edited|rejected)
   â”œâ”€ Extract findings_detail & reviewer_stats for each
   â”‚
   â–¼

COMPUTE REVIEWER EFFECTIVENESS:
   â”œâ”€ Per role, calculate:
   â”‚  â”œâ”€ proposed: count of findings this role suggested
   â”‚  â”œâ”€ accepted: count of findings user approved
   â”‚  â”œâ”€ rejected: count of findings user rejected
   â”‚  â”œâ”€ precision: accepted / proposed (0.0-1.0)
   â”‚  â””â”€ outcome_correlation: reviews with >=1 accepted AND outcome in (approved|edited)
   â”‚
   â”œâ”€ Example results:
   â”‚  â”œâ”€ security:    proposed=10, accepted=9, precision=0.90 âœ“
   â”‚  â”œâ”€ testing:     proposed=8,  accepted=2, precision=0.25 âœ— LOW
   â”‚  â”œâ”€ clarity:     proposed=6,  accepted=1, precision=0.17 âœ— LOW
   â”‚  â””â”€ domain_sme:  proposed=12, accepted=10, precision=0.83 âœ“
   â”‚
   â–¼

IDENTIFY OVER/UNDERPERFORMERS:
   â”œâ”€ HIGH precision roles (> 0.70): trusted, increase weight
   â”œâ”€ LOW precision roles (< 0.70): needs improvement, decrease weight
   â”‚
   â–¼

DISPLAY EFFECTIVENESS DASHBOARD:
   â”œâ”€ Shows: "Reviewer Effectiveness"
   â”œâ”€ Lists all roles with precision % and (accepted/proposed)
   â”œâ”€ Marks below-threshold with "â† below threshold"
   â”‚
   â–¼ (Preview only, no changes yet)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 2: WEIGHT ADAPTATION (via `node adapt.cjs --apply`)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

COMPUTE WEIGHT SUGGESTIONS:
   â”‚
   â”œâ”€ For each reviewer with >= min_reviews (default: 5):
   â”‚  â”œâ”€ new_weight = current_weight * (reviewer_precision / avg_precision)
   â”‚  â”œâ”€ Clamp to [0.5, 3.0] range
   â”‚  â”œâ”€ Round to 2 decimal places
   â”‚  â”‚
   â”‚  â”œâ”€ Example:
   â”‚  â”‚  current: { security: 1.2, testing: 0.8, clarity: 0.9, domain_sme: 1.5 }
   â”‚  â”‚  avg_precision: (0.90 + 0.25 + 0.17 + 0.83) / 4 = 0.5375
   â”‚  â”‚  suggested: {
   â”‚  â”‚    security:    1.2 * (0.90 / 0.5375) = 2.01 â†’ 2.0  (increase)
   â”‚  â”‚    testing:     0.8 * (0.25 / 0.5375) = 0.37 â†’ 0.5  (clamped)
   â”‚  â”‚    clarity:     0.9 * (0.17 / 0.5375) = 0.28 â†’ 0.5  (clamped)
   â”‚  â”‚    domain_sme:  1.5 * (0.83 / 0.5375) = 2.31 â†’ 2.31 (increase)
   â”‚  â”‚  }
   â”‚  â”‚
   â”‚
   â–¼

APPLY CHANGES (if user confirms `--apply`):
   â”‚
   â”œâ”€ Save previous weights to config.weights_history (FIFO, max 10)
   â”œâ”€ Write new weights to config.json
   â”œâ”€ NEXT RUN uses updated weights for composite scoring
   â”‚
   â–¼

EFFECT:
   â”œâ”€ Next 5+ reviews will show:
   â”‚  â”œâ”€ security findings weighted MORE heavily (weight 2.0 vs 1.2)
   â”‚  â”œâ”€ testing findings weighted LESS (weight 0.5 vs 0.8)
   â”‚  â”œâ”€ clarity findings weighted LESS (weight 0.5 vs 0.9)
   â”‚  â””â”€ domain_sme findings weighted MORE (weight 2.31 vs 1.5)
   â”‚
   â”œâ”€ Results:
   â”‚  â”œâ”€ High-precision security reviews dominate â†’ fewer false positives
   â”‚  â”œâ”€ Low-precision testing/clarity reviews have less influence
   â”‚  â”œâ”€ System gravitates toward reviewers that users actually approve
   â”‚
   â–¼

CONTINUOUS CYCLE:
   â”œâ”€ More reviews accumulate â†’ more data for effectiveness analysis
   â”œâ”€ Can run `/prompt-review:adapt 60 --apply` every month
   â”œâ”€ Weights continuously improve based on user feedback
   â”œâ”€ System evolves toward YOUR preferences and patterns

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHASE 3: POLICY LEARNING (Offline Prompt Improvement)                       â”‚
â”‚         [Phase 3 only, gated behind debate.enabled=true]                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

WHEN DEBATE IS ENABLED:
   â”‚
   â”œâ”€ Each debate round produces judge feedback:
   â”‚  â”œâ”€ argument_quality_score (0-10)
   â”‚  â”œâ”€ argument_labels (["specific", "evidence-backed", ...])
   â”‚  â””â”€ policy_signal ("This role should emphasize X" or "Consider Y")
   â”‚
   â”œâ”€ Judge feedback accumulated in debate_log
   â”œâ”€ Stored in audit entries for historical analysis
   â”‚
   â–¼

RUN POLICY ANALYSIS (via Python script, offline):
   â”‚
   â”œâ”€ Aggregate debate feedback from last 30 days
   â”œâ”€ For each role:
   â”‚  â”œâ”€ avg_argument_quality: average quality score across debates
   â”‚  â”œâ”€ common_labels: most frequent argument labels
   â”‚  â”œâ”€ policy_signal: common insights from judges
   â”‚  â””â”€ needs_update: if quality is low or signals are clear
   â”‚
   â”œâ”€ Example output:
   â”‚  â”œâ”€ security:
   â”‚  â”‚  â”œâ”€ avg_quality: 8.2/10
   â”‚  â”‚  â”œâ”€ labels: ["specific", "evidence-backed"]
   â”‚  â”‚  â”œâ”€ signal: "Maintain current calibration, very good"
   â”‚  â”‚  â””â”€ needs_update: false
   â”‚  â”‚
   â”‚  â”œâ”€ testing:
   â”‚  â”‚  â”œâ”€ avg_quality: 4.1/10
   â”‚  â”‚  â”œâ”€ labels: ["vague", "speculative"]
   â”‚  â”‚  â”œâ”€ signal: "Add specific examples to test case requirements"
   â”‚  â”‚  â””â”€ needs_update: true
   â”‚  â”‚
   â”‚
   â–¼

GENERATE PROMPT PROPOSALS:
   â”‚
   â”œâ”€ For roles marked needs_update:
   â”‚  â”œâ”€ Read current SYSTEM_PROMPT from reviewers/<role>.cjs
   â”‚  â”œâ”€ Send to Claude Sonnet with policy_signal + past quality feedback
   â”‚  â”œâ”€ Sonnet proposes improved prompt
   â”‚  â””â”€ Write to reviewers/prompts/<role>.txt (human review required!)
   â”‚
   â”œâ”€ NO automatic updates to .cjs files (safety first)
   â”œâ”€ Human must review proposal and decide to adopt
   â”‚
   â–¼

HUMAN REVIEW & ADOPTION:
   â”‚
   â”œâ”€ Read reviewers/prompts/<role>.txt for proposal
   â”œâ”€ Compare with current reviewers/<role>.cjs
   â”œâ”€ If good: copy new content to .cjs file
   â”œâ”€ Commit & deploy
   â”‚
   â”œâ”€ Effect:
   â”‚  â”œâ”€ Testing role now emphasizes examples in its prompts
   â”‚  â”œâ”€ Next reviews from testing are more specific
   â”‚  â”œâ”€ User acceptance of testing findings increases
   â”‚  â””â”€ Cycle continues...
   â”‚
   â–¼

THE VIRTUOUS CYCLE:
   â”œâ”€ More reviews â†’ better effectiveness data
   â”œâ”€ Better data â†’ smarter weight adaptation
   â”œâ”€ Debates (if enabled) â†’ quality insights
   â”œâ”€ Quality insights â†’ improved role prompts
   â”œâ”€ Better prompts â†’ higher user acceptance
   â”œâ”€ Higher acceptance â†’ higher effectiveness scores
   â”œâ”€ The system continuously improves without code changes!

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                  END OF LEARNING LOOP (Repeats Every Month)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Knowledge Flow: How Learning is Captured & Applied

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KNOWLEDGE CAPTURE & EVOLUTION                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LAYER 1: RAW FINDINGS (Per-Review Data)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Audit Log Entry (findings_detail):                                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ {                                                                    â”‚  â”‚
â”‚ â”‚   "reviewer_role": "security",                                       â”‚  â”‚
â”‚ â”‚   "finding_id": "SEC-001",                                           â”‚  â”‚
â”‚ â”‚   "severity": "blocker",                                             â”‚  â”‚
â”‚ â”‚   "issue": "Prompt allows reading .env without validation",          â”‚  â”‚
â”‚ â”‚   "op": "AddGuardrail",                                              â”‚  â”‚
â”‚ â”‚   "target": "constraints"                                            â”‚  â”‚
â”‚ â”‚ }                                                                    â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â†“ (user clicks "accept" or "reject")                                      â”‚
â”‚ "suggestions_accepted": ["SEC-001", "TST-003"]                            â”‚
â”‚ "suggestions_rejected": ["CLR-005"]                                       â”‚
â”‚ â†“ (recorded in same audit entry via updateAuditOutcome)                   â”‚
â”‚ "reviewer_stats": {                                                       â”‚
â”‚   "security": { "proposed": 3, "accepted": 2, "rejected": 1 },            â”‚
â”‚   "testing": { "proposed": 2, "accepted": 1, "rejected": 0 }             â”‚
â”‚ }                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LAYER 2: PATTERN EXTRACTION (Across Multiple Reviews)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ computeTopPatterns(entries) â€” Phase 1, stats.cjs:                          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Scans all findings_detail across 30-day window:                      â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ Issue: "Missing error handling for async"                           â”‚  â”‚
â”‚ â”‚   â”œâ”€ Run 1: testing found it (rejected)                             â”‚  â”‚
â”‚ â”‚   â”œâ”€ Run 2: testing found it (accepted)                             â”‚  â”‚
â”‚ â”‚   â”œâ”€ Run 3: testing found it (accepted)                             â”‚  â”‚
â”‚ â”‚   â”œâ”€ Run 4: domain_sme found it (accepted)                          â”‚  â”‚
â”‚ â”‚   â”œâ”€ Run 5: testing found it (rejected)                             â”‚  â”‚
â”‚ â”‚   â””â”€ Aggregate: 5 total, 3 accepted, 2 rejected                     â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ Result: "Missing error handling" is a TOP PATTERN (5 occurrences)   â”‚  â”‚
â”‚ â”‚         Especially from testing (appears 4/5 times)                 â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â†“ (displayed in `node index.cjs --stats`)                                 â”‚
â”‚ Top Patterns:                                                             â”‚
â”‚   1. "Missing error handling for async" â€” 5 occurrences (testing: 4)     â”‚
â”‚   2. "Unclear requirement language" â€” 4 occurrences (clarity: 4)         â”‚
â”‚   3. "Missing input validation" â€” 3 occurrences (security: 3)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LAYER 3: EFFECTIVENESS METRICS (Reviewer Performance)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ computeReviewerEffectiveness(entries) â€” Phase 2, stats.cjs:               â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Per-reviewer aggregation of reviewer_stats:                          â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ security:                                                            â”‚  â”‚
â”‚ â”‚   â”œâ”€ proposed: 12 findings across all reviews                        â”‚  â”‚
â”‚ â”‚   â”œâ”€ accepted: 10 findings user approved                             â”‚  â”‚
â”‚ â”‚   â”œâ”€ rejected: 2 findings user rejected                              â”‚  â”‚
â”‚ â”‚   â””â”€ precision: 10/12 = 0.83 (83%)                                   â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ testing:                                                             â”‚  â”‚
â”‚ â”‚   â”œâ”€ proposed: 8 findings                                            â”‚  â”‚
â”‚ â”‚   â”œâ”€ accepted: 2 findings                                            â”‚  â”‚
â”‚ â”‚   â”œâ”€ rejected: 6 findings                                            â”‚  â”‚
â”‚ â”‚   â””â”€ precision: 2/8 = 0.25 (25%) â† LOW!                              â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â†“ (displayed in dashboard + used for weight adaptation)                   â”‚
â”‚ Dashboard:                                                                 â”‚
â”‚   Reviewer Effectiveness                                                  â”‚
â”‚     security:    precision 0.83  (10/12 accepted)                         â”‚
â”‚     testing:     precision 0.25  (2/8 accepted)  â† below threshold        â”‚
â”‚     clarity:     precision 0.33  (2/6 accepted)  â† below threshold        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LAYER 4: WEIGHT ADAPTATION (System Recalibration)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ computeWeightSuggestions(reviewerMetrics, currentWeights) â€” Phase 2:       â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Before:  { security: 1.0, testing: 1.0, clarity: 1.0, ... }         â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ Effectiveness: security 0.83, testing 0.25, clarity 0.33             â”‚  â”‚
â”‚ â”‚ avg_precision: (0.83 + 0.25 + 0.33) / 3 = 0.47                      â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ Suggested weights:                                                   â”‚  â”‚
â”‚ â”‚   security:  1.0 * (0.83 / 0.47) = 1.77 (increase â€” high precision) â”‚  â”‚
â”‚ â”‚   testing:   1.0 * (0.25 / 0.47) = 0.53 (decrease â€” low precision)  â”‚  â”‚
â”‚ â”‚   clarity:   1.0 * (0.33 / 0.47) = 0.70 (decrease â€” low precision)  â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ After (if --apply): { security: 1.77, testing: 0.53, clarity: 0.70 }â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ Saved to: config.json + config.weights_history                      â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â†“ (affects composite_score calculation in next runs)                      â”‚
â”‚ Next runs composite_score = weighted average using new weights:           â”‚
â”‚   BEFORE: (8 + 5 + 7 + 6) / 4 = 6.5                                      â”‚
â”‚   AFTER:  (8Ã—1.77 + 5Ã—0.53 + 7Ã—0.70 + 6Ã—1.0) / (1.77+0.53+0.70+1.0)     â”‚
â”‚        = (14.16 + 2.65 + 4.9 + 6.0) / 4.0 = 6.93                         â”‚
â”‚   â†‘ Security pulls up score; testing/clarity have less influence          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LAYER 5: DEBATE INSIGHTS (Quality Signals for Prompt Evolution)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Judge Feedback â€” Phase 3, judge.cjs (if debate.enabled=true):             â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Debate between security and testing over:                            â”‚  â”‚
â”‚ â”‚   Conflict: Should prompts allow env variable access?               â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ Judge feedback:                                                      â”‚  â”‚
â”‚ â”‚ â”œâ”€ security:                                                        â”‚  â”‚
â”‚ â”‚ â”‚  â”œâ”€ argument_quality_score: 8.2/10                                 â”‚  â”‚
â”‚ â”‚ â”‚  â”œâ”€ argument_labels: ["specific", "evidence-backed"]              â”‚  â”‚
â”‚ â”‚ â”‚  â””â”€ policy_signal: "Excellent calibration. Maintain current       â”‚  â”‚
â”‚ â”‚ â”‚                     emphasis on least-privilege principle."        â”‚  â”‚
â”‚ â”‚ â”‚                                                                    â”‚  â”‚
â”‚ â”‚ â””â”€ testing:                                                         â”‚  â”‚
â”‚ â”‚    â”œâ”€ argument_quality_score: 4.1/10                                 â”‚  â”‚
â”‚ â”‚    â”œâ”€ argument_labels: ["vague", "speculative", "missing examples"] â”‚  â”‚
â”‚ â”‚    â””â”€ policy_signal: "Add concrete test case examples to your       â”‚  â”‚
â”‚ â”‚                       arguments. Currently too abstract."            â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ Stored in: audit_log.debate_log.judge_feedback                      â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â†“ (aggregated offline via policy analysis)                                â”‚
â”‚ computePolicyInsights(days):                                              â”‚
â”‚   testing:                                                                â”‚
â”‚     â”œâ”€ avg_argument_quality: 4.1/10                                       â”‚
â”‚     â”œâ”€ common_labels: ["vague", "speculative"]                           â”‚
â”‚     â”œâ”€ policy_signal: "Add examples to test requirement descriptions"    â”‚
â”‚     â””â”€ needs_update: true                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LAYER 6: PROMPT EVOLUTION (Policy Learning Output)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ generatePromptProposal() â€” Phase 3, policy.cjs:                            â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚ â”‚ Input:                                                               â”‚  â”‚
â”‚ â”‚   current SYSTEM_PROMPT (from reviewers/testing.cjs):               â”‚  â”‚
â”‚ â”‚   "Identify gaps in test coverage, suggest test cases..."           â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚   policy_signal: "Add concrete test case examples to arguments"     â”‚  â”‚
â”‚ â”‚   avg_quality: 4.1/10 (needs improvement)                           â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ â†’ Send to Claude Sonnet with context                                â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ Output (Proposed SYSTEM_PROMPT):                                    â”‚  â”‚
â”‚ â”‚   "Identify gaps in test coverage. Suggest SPECIFIC test cases     â”‚  â”‚
â”‚ â”‚    with examples: include setup, assertion, expected output.       â”‚  â”‚
â”‚ â”‚    Always reference specific lines or code patterns from the       â”‚  â”‚
â”‚ â”‚    prompt that your test would validate..."                         â”‚  â”‚
â”‚ â”‚                                                                      â”‚  â”‚
â”‚ â”‚ Written to: reviewers/prompts/testing.txt (human review required)   â”‚  â”‚
â”‚ â”‚ NOT automatically written to reviewers/testing.cjs (safety!)        â”‚  â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚ â†“ (human decides to adopt proposal)                                       â”‚
â”‚ Developer approves proposal:                                              â”‚
â”‚   â”œâ”€ Read reviewers/prompts/testing.txt                                  â”‚
â”‚   â”œâ”€ Compare with reviewers/testing.cjs                                  â”‚
â”‚   â”œâ”€ Copy new content to testing.cjs                                     â”‚
â”‚   â”œâ”€ Commit & deploy                                                     â”‚
â”‚                                                                           â”‚
â”‚ Next run with improved testing prompt:                                    â”‚
â”‚   â”œâ”€ Testing role suggests: "Add test case: describe API response       â”‚
â”‚   â”‚   when database returns 500 error. Assert client retries or         â”‚
â”‚   â”‚   logs error appropriately."                                         â”‚
â”‚   â”œâ”€ Much more specific than before!                                     â”‚
â”‚   â”œâ”€ User more likely to accept (precision increases)                    â”‚
â”‚   â”œâ”€ Positive feedback loop begins                                       â”‚
â”‚   â””â”€ Cycle repeats...                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     KNOWLEDGE LOOP SUMMARY                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Raw Data (findings per review)
            â†“
Pattern Recognition (top issues across reviews)
            â†“
Effectiveness Metrics (reviewer performance)
            â†“
Weight Adaptation (recalibrate importance)
            â†“
Debate Insights (quality signal from judge)
            â†“
Prompt Evolution (improve reviewer instructions)
            â†“
Better Recommendations (next review cycle)
            â†“
Higher User Acceptance
            â†“
Better Effectiveness Metrics (feedback loop)
            â†“
MORE WEIGHT on effective reviewers
            â†“
[CYCLE REPEATS â€” System continuously improves without code changes]
```

---

## Three-Phase Learning System Overview

```
PHASE 1: AUDIT LOGGING (Foundation)
â”œâ”€ What: Record detailed findings, reviewer stats, user decisions
â”œâ”€ Why: Need data to learn from
â”œâ”€ Output: Audit logs with findings_detail + acceptance tracking
â”œâ”€ Impact: Enables all downstream learning features
â””â”€ Status: âœ… SHIPPED (Phase 1)

PHASE 2: GEA REFLECTION (Continuous Adaptation)
â”œâ”€ What: Analyze audit logs, compute reviewer effectiveness, adapt weights
â”œâ”€ Why: System should trust high-precision reviewers more
â”œâ”€ Output: Weight suggestions + stats dashboard
â”œâ”€ Command: `node adapt.cjs [days] [--apply]`
â”œâ”€ Skill: `/prompt-review:adapt`
â”œâ”€ Impact: Next runs use better weights automatically
â””â”€ Status: âœ… SHIPPED (Phase 2)

PHASE 3: COMAS DEBATE (Offline Policy Learning)
â”œâ”€ What: Debate conflicts, judge quality, extract policy signals
â”œâ”€ Why: Disagreements are learning opportunities
â”œâ”€ Output: Debate logs + judge feedback + prompt proposals
â”œâ”€ Config: debate.enabled (default: false, opt-in)
â”œâ”€ Impact: Reviewer prompts improve, higher quality suggestions
â””â”€ Status: âœ… SHIPPED (Phase 3)
```

---

## Directory Structure & File Locations

```
~/git/prompt-review/
â”œâ”€ index.cjs                   # Main entry point (hook/skill handlers, runFullPipeline)
â”œâ”€ orchestrator.cjs            # Fan-out logic (runReviewersApi)
â”œâ”€ editor.cjs                  # Merge logic (mergeCritiques, computeCompositeScore)
â”œâ”€ renderer.cjs                # Presentation (renderEditingBlock)
â”œâ”€ cost.cjs                    # PHASE 1: Audit logging, outcome tracking
â”œâ”€ stats.cjs                   # Analytics dashboard (generateStats, PHASE 2: effectiveness)
â”œâ”€ reflection.cjs              # PHASE 2: generateReflectionReport, computeWeightSuggestions
â”œâ”€ adapt.cjs                   # PHASE 2: CLI tool (previewAdaptation, applyAdaptation)
â”œâ”€ debate.cjs                  # PHASE 3: selectDebatePairs, runDebatePhase
â”œâ”€ judge.cjs                   # PHASE 3: runJudge, feedback extraction
â”œâ”€ policy.cjs                  # PHASE 3: readCurrentSystemPrompt, generatePromptProposal
â”œâ”€ config.json                 # Configuration (reviewer weights, debate settings, reflection settings)
â”œâ”€ reviewers/                  # LLM prompts for each specialist role
â”‚  â”œâ”€ security.cjs             # Security-focused SYSTEM_PROMPT
â”‚  â”œâ”€ testing.cjs              # Testing-focused SYSTEM_PROMPT
â”‚  â”œâ”€ clarity.cjs              # Clarity-focused SYSTEM_PROMPT
â”‚  â”œâ”€ domain-sme.cjs           # Domain expertise SYSTEM_PROMPT
â”‚  â”œâ”€ frontend-ux.cjs          # Frontend/UX SYSTEM_PROMPT (optional)
â”‚  â”œâ”€ documentation.cjs        # Documentation SYSTEM_PROMPT (optional)
â”‚  â””â”€ prompts/                 # PHASE 3: Human-reviewable prompt proposals (gitignored)
â”‚     â”œâ”€ testing.txt           # Proposed SYSTEM_PROMPT for testing role (auto-generated)
â”‚     â””â”€ [other roles].txt     # Other role proposals
â”œâ”€ logs/                       # PHASE 1: Audit logs (gitignored)
â”‚  â”œâ”€ 20240225.jsonl           # Feb 25 reviews (one JSON line per review)
â”‚  â”œâ”€ 20240226.jsonl           # Feb 26 reviews
â”‚  â””â”€ [YYYYMMDD].jsonl         # One file per day
â”œâ”€ skills/
â”‚  â”œâ”€ review/SKILL.md          # /prompt-review:review skill (fan-out review)
â”‚  â”œâ”€ stats/SKILL.md           # /prompt-review:stats skill (show dashboard)
â”‚  â””â”€ adapt/SKILL.md           # /prompt-review:adapt skill (preview/apply weights)
â”œâ”€ .claude/                    # Agent-facing documentation
â”‚  â”œâ”€ CLAUDE.md                # Development rules
â”‚  â”œâ”€ ARCHITECTURE.md          # This file
â”‚  â”œâ”€ phase-1-audit-logging.md # Phase 1 spec
â”‚  â”œâ”€ phase-2-gea-reflection.md# Phase 2 spec
â”‚  â””â”€ phase-3-comas-debate.md  # Phase 3 spec
â”œâ”€ tests/                      # All test files (12 tests, all passing)
â”‚  â”œâ”€ run.cjs                  # Test runner
â”‚  â”œâ”€ audit-schema.test.cjs    # Phase 1 tests (5)
â”‚  â”œâ”€ reflection.test.cjs      # Phase 2 tests (6)
â”‚  â”œâ”€ adapt.test.cjs           # Phase 2 tests (5)
â”‚  â”œâ”€ debate.test.cjs          # Phase 3 tests (5)
â”‚  â”œâ”€ judge.test.cjs           # Phase 3 tests (3)
â”‚  â””â”€ policy.test.cjs          # Phase 3 tests (3)
â”œâ”€ package.json                # npm config
â”œâ”€ .nvmrc                      # Node version (22)
â”œâ”€ .gitignore                  # Ignore logs/, reviewers/prompts/, node_modules/
â”œâ”€ config.json                 # Configuration (weights, reflection settings, debate settings)
â””â”€ README.md                   # (To be created) User-facing documentation

~/.claude/plugins/prompt-review/   # Symlink to ~/git/prompt-review (for Claude Code)
```

---

## How the Learning System Scales

```
Week 1:  5 reviews â†’ 2 days of data â†’ insufficient for adaptation (need >=5)
Week 2:  10 reviews â†’ 7 days of data â†’ can analyze patterns, suggest weights
Week 3:  15 reviews â†’ 14 days of data â†’ strong data for weight adaptation
Week 4:  20+ reviews â†’ 30 days of data â†’ stable metrics, ready to apply

Monthly cadence:
  â”œâ”€ Day 1-30: Accumulate reviews
  â”œâ”€ Day 31: Run `node adapt.cjs 30` to preview weight suggestions
  â”œâ”€ Day 32: Review suggestions, decide on adoption
  â”œâ”€ Day 33: Run `node adapt.cjs 30 --apply` if desired
  â”œâ”€ Day 34+: Next month's reviews use improved weights
  â””â”€ Repeat

With debate enabled (optional):
  â”œâ”€ Debates accumulate judge feedback
  â”œâ”€ Monthly: Run policy analysis to identify prompt improvements
  â”œâ”€ Offline: Sonnet generates prompt proposals
  â”œâ”€ Manual: Human reviews and adopts improvements
  â”œâ”€ Effect: Reviewer prompts evolve based on actual usage patterns
  â””â”€ Virtuous cycle: better prompts â†’ better suggestions â†’ higher acceptance
```

---

## Key Insights

1. **Audit logs are everything** â€” Phase 1 creates the foundation. Without detailed findings_detail + acceptance tracking, nothing else works.

2. **Weights adapt to user behavior** â€” The system doesn't need to "know" which reviewer is best. It learns from watching which reviewers the user actually approves.

3. **Debate is optional but powerful** â€” With debate disabled, you still get weight adaptation. With debate enabled, you also get prompt evolution.

4. **No code changes needed for improvement** â€” All three phases improve the system without modifying the codebase. You just change config.json or deploy new reviewer prompts.

5. **The feedback loop closes** â€” User decision (accept/reject) â†’ effectiveness metric â†’ weight adjustment â†’ better next time. It's a self-improving system.

6. **Safety through human review** â€” Policy proposals go to .txt files for human review before touching .cjs. Debate doesn't affect merge output.

7. **Deterministic learning** â€” All math is deterministic (no randomness), so weight adaptation is reproducible and debuggable.
